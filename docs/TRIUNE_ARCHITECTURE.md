## Triune Perceptual Learner

System Architecture and Design Document

This document is a consolidated "north star" spec for the Triune Perceptual Learner.
It integrates predictive coding, Hebbian graphs, trace learning, and developmental stages
into a single engineering blueprint.

Project name: Triune Perceptual Learner  
Goal: Build an online, self-supervised vision system that evolves from "Insect-like"
reflexes to "Mammalian" semantic understanding.  
Core architecture: Hybrid neuro-symbolic (spiking/graph + deep learning).

---

### I. Core Philosophy

The system does not learn from labeled datasets. It learns from physics and time.

- Reflex First: Hard-coded primitives (looming/motion) act as the "teacher."
- Predictive Loop: The system minimizes surprise. If it can predict an object, it stops
  processing it (habituation).
- Developmental Staging: Complexity unlocks in tiers:
  Physics -> Object Unity -> Animacy -> Species Classification.

---

### II. System Architecture (The 3-Tier Stack)

#### Tier 1: The "Insect" Scaffold (Hard-Coded Reflexes)

- Role: Attention mechanism and survival. Runs on raw pixels.
- Input: Frame difference (motion) + optical flow.
- Primitives to code:
  - Looming detector: if Expansion_Rate > Threshold: label = "Obstacle"
  - Common fate: group pixels moving with identical velocity vectors
  - Gravity bias: flag objects moving up or accelerating laterally as "Potential Agents"
- Output: Regions of interest (ROIs). It tells the upper layers where to look.

#### Tier 2: The Deep Hebbian Graph (Perceptual Learning)

- Role: Spatiotemporal binding and invariance.
- Architecture: Predictive coding network (PCN).
- Mechanism:
  - Trace learning: neurons have a memory tail (leaky integrator). If input A leads
    to input B, they wire together.
  - The "Singular" (Token): a dynamic slot that binds features into one object file.
  - The "Lob" (Top-Down): the token projects a prediction down to the pixel layer.
  - Error minimization: only the difference (Input - Prediction) is propagated.
- Output: Stable, invariant object representations (e.g., "Rotating Red Object").

#### Tier 3: The "Mammal" Head (Semantic Classification)

- Role: Categorization and strategy.
- Architecture: Modified YOLO (You Only Look Once).
- Integration:
  - Does not scan the whole image.
  - Only scans the Error Maps (surprise regions) generated by Tier 2.
  - Adaptive resolution:
    - If Object_Value == Low: output generic class ("Tree")
    - If Object_Value == High: output specific instance ("My Apple Tree")

---

### III. The "Integrated Shiloh Model" (Logic Flow)

#### 1. The Processing Loop

For every frame t:

- Reflex Check (Tier 1):
  - Calculate optical flow.
  - Detect looming or fast lateral motion.
  - Decision: Is this region dangerous or interesting? If no, ignore. If yes, pass ROI to Tier 2.
- Predictive Coding (Tier 2):
  - Top-Down: active object tokens "lob" a prediction of what the ROI should look like at t.
  - Bottom-Up: compare prediction vs. reality.
  - Habituation:
    - If Error ~ 0: silence (do nothing, save energy).
    - If Error > Threshold: surprise spike. Wake up the object file. Update internal representation.
- Semantic Check (Tier 3 - YOLO):
  - Triggered only by surprise spikes.
  - YOLO classifies the object causing the error.
  - Feedback: YOLO labels act as a "High Value" flag, forcing Tier 2 to memorize specifics.

#### 2. The Learning Rule (Modified Hebbian)

For the graph (Tier 2), use trace-based Oja's rule:

- Pre_trace: moving average of the input (bridges time gaps).
- Decay: prevents weight explosion (normalization).
- Effect: pixels that fire in sequence (motion) get wired into the same neuron.

---

### IV. Implementation Roadmap for Agent

#### Phase 1: The "Blind Fly" (Physics Only)

- Task: Build the InsectVision class.
- Goal: The agent moves in a 2D space avoiding collisions using only optical flow expansion.
- No neural net yet. Pure OpenCV/math algorithms.

#### Phase 2: The "Dreaming Machine" (Predictive Coding)

- Task: Build the PredictiveObjectSlot class (PyTorch).
- Goal: Show the agent a simple shape (e.g., a rotating triangle).
- Success metric: error energy drops to near zero after 50 frames (habituation). If the shape
  changes, energy should spike.

#### Phase 3: The "YOLO Hybrid" (Semantics)

- Task: Wrap a Tiny-YOLO model.
- Logic:
  - Input: Error_Map from Phase 2 (black image with white pixels where prediction failed).
  - Training: Self-supervised. If Tier 1 detects looming, label the object "Obstacle" and
    train YOLO on that crop.

---

### V. Consolidated Code Snippets (Python/PyTorch)

#### A. The Predictive Unit (The "Object File")

import torch
import torch.nn as nn

class BioPredictiveUnit(nn.Module):
    def __init__(self, input_dim, internal_dim):
        super().__init__()
        # The "Singular" (Internal Representation / Token)
        self.state = torch.zeros(1, internal_dim, requires_grad=True)
        # The "Lob" Generator (Decoder)
        self.decoder = nn.Linear(internal_dim, input_dim)

    def forward(self):
        # Generate Top-Down Prediction
        return self.decoder(self.state)

    def process_sensation(self, sensory_input, learning_rate=0.1):
        """
        The core loop: Prediction -> Error -> Adaptation
        """
        # 1. Predict
        prediction = self.forward()

        # 2. Calculate Surprise (Error)
        error = sensory_input - prediction
        energy = torch.sum(error ** 2)

        # 3. Habituation Logic
        if energy < 0.01:
            return energy.item(), "HABITUATED"  # Stop processing

        # 4. Adaptation (Fast Inference)
        # Update the "Singular" state to match reality
        energy.backward()
        with torch.no_grad():
            self.state -= learning_rate * self.state.grad
            self.state.grad.zero_()

        return energy.item(), "UPDATING"

#### B. The Trace Learning Layer (Temporal Binding)

import numpy as np

class TemporalHebbianLayer:
    def __init__(self, n_in, n_out, trace_decay=0.5):
        self.W = np.random.randn(n_out, n_in) * 0.01
        self.input_trace = np.zeros(n_in)
        self.decay = trace_decay

    def step(self, x_input):
        # 1. Update Trace (Smear input over time)
        self.input_trace = (1 - self.decay) * self.input_trace + (self.decay * x_input)

        # 2. Activation
        y = np.dot(self.W, x_input)

        # 3. Hebbian Update (Wire to the Trace, not the instant input)
        # Delta_W = Learning_Rate * (Output * Input_Trace)
        # Note: Add Oja's rule normalization here in production
        d_W = 0.01 * np.outer(y, self.input_trace)
        self.W += d_W

        return y

#### C. The Adaptive Resolution Gate

def adaptive_resolution_gate(object_class, species_value_map):
    """
    Decides if we should save the object as a specific instance
    or just a generic class.
    """
    # 1. Get Value (Necessity)
    value = species_value_map.get(object_class, 0.0)

    # 2. Threshold Check
    if value > 0.8:  # High Value (e.g., Predator)
        # Enable High-Res Encoding (Save Residuals)
        return "DISCRIMINATE_PARTICULARS"
    # Low Value (e.g., Rock)
    # Collapse to Prototype
    return "COLLAPSE_TO_GENERIC"
